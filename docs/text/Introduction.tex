In recent years, convolutional neural networks (CNNs) have emerged as a cornerstone technology in computer vision and broader industrial applications, powering innovations in areas such as autonomous vehicles, facial recognition, and quality inspection \cite{lecun2015deep, krizhevsky2012imagenet}. By leveraging convolutional layers to capture local spatial patterns, pooling operations to reduce dimensionality, and fully connected layers to integrate learned features, CNNs have demonstrated exceptional performance across a wide range of classification, detection, and segmentation tasks \cite{he2016deep}. Their capacity to automatically learn hierarchical features from raw input data makes them particularly well-suited for complex image-based challenges.
\\
\\
In parallel, classical approaches such as Logistic Regression offer a useful baseline for understanding dataset complexity and benchmarking the performance gains afforded by more sophisticated neural architectures \cite{hastie2009elements}. While Logistic Regression alone is not typically suited for image classification, comparing it against CNNs helps illuminate the advantages of nonlinear feature extraction and deep representation learning. To further explore the performance of Logistic Regression on classification tasks, we experiment with multiple hyperparameters. Specifically, we test five distinct learning rates, batch sizes, and numbers of epochs to assess how these variations influence the model's classification accuracy and convergence behavior.
\\
\\
In this study, we focus on the well-known MNIST dataset, a benchmark collection of handwritten digit images that serves as a standardized platform for evaluating classification models \cite{lecun1998gradient}. We employ a systematic approach to hyperparameter optimization by conducting a comprehensive grid search over CNN architectures and their key hyperparameters. By exploring various configurations, such as differing numbers of convolutional filters, kernel sizes, dropout rates, pooling, padding and network depths, we aim to identify the CNN architecture that yields the best classification accuracy on MNIST.
\\
\\
Our evaluation metrics center on classification accuracy, allowing us to directly compare the predictive power of our best-performing CNN against the best-performing Logistic Regression model. We also discuss the computational considerations associated with both approaches, as modern industrial applications often require methods that are both accurate and efficient.
\\
\\
This article is structured as follows: We begin by introducing the MNIST dataset and detailing the preprocessing steps taken to ensure data quality and consistency. We then outline the architectural components of our CNN and the rationale behind their effectiveness in image classification tasks. We next define our baseline Logistic Regression model and the variations introduced in learning rates, batch sizes, and epochs to investigate their impact on performance. Subsequently we describe the K-fold cross validation and bootstrap algorithms which we use for model validation and evaluation. Following this, we describe the grid search procedure employed to explore the hyperparameter space of CNN architectures and linear configurations. Subsequently, we present and analyze the results, highlighting how different CNN architectures and hyperparameters compare against the various Logistic Regression configurations. Finally, we summarize our findings, discuss their implications for real-world industrial contexts, and suggest directions for future researchs.




