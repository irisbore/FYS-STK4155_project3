In this study, we compared the performance of Convolutional Neural Networks (CNNs) and Logistic Regression for digit classification on the MNIST dataset. Through grid search and hyperparameter optimization, we found that CNNs outperformed Logistic Regression, achieving an average accuracy of 98.9\%, compared to 92.0\% for Logistic Regression. These results highlight the importance of nonlinear feature extraction in image classification tasks, with CNNs being essential for applications that demand near-perfect accuracy. Key insights include the role of padding and pooling in reducing computational costs, as well as the minimal impact of dropout rates and activation functions under the specific experimental conditions. However, the complexity of CNNs and their reduced interpretability present challenges. The findings also underscore the trade-off between computational efficiency and model complexity. This study shows that achieving the optimal performance when tuning networks with many parameters, while at the same ensuring that the process is reproducible, requires effective methods. While it is clear that CNNs clearly outperform Logistic Regression on image recognition, future work will explore more efficient tuning methods, such as Bayesian optimization, to streamline the tuning process and further improve performance.
