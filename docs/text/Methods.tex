This study primarily focused on implementing and comparing the performance of various Convolutional Neural Networks (CNNs) with different layers, kernel sizes, numbers of filters, activation functions, and regularization techniques on the MNIST dataset. To illustrate the advantages of CNNs, we benchmarked our results against a Logistic Regression (LR) model tuned with five different learning rates, batch sizes, and numbers of epochs. To ensure the robustness and reliability of our findings, we employed several data validation and resampling techniques, including train-test splitting, cross-validation on the training set, and bootstrapping on the test set.                


\subsection{Data Loading and Preprocessing}

\subsubsection{Train-Test Split}

We used PyTorch to load the MNIST dataset, which comprises 70,000 images in total. This dataset is pre-divided into a training set of 60,000 images and a test set of 10,000 images. During the optimization process, the training set was utilized to iteratively update the model parameters in order to minimize the loss function. Meanwhile, the test set remained completely untouched until the final evaluation phase. This separation allowed us to evaluate the model's ability to generalize to new, unseen data. By conducting model selection and hyperparameter tuning exclusively on the training data, and reserving final performance checks for the test data, we obtained a more realistic measure of the modelâ€™s predictive power and robustness.

\subsubsection{Feature Scaling}
When loading our data set, we scaled the pixels to be within the range [0, 1]. This normalization ensured that the input features (pixels) lied within a more uniform and manageable range than the original pixel range of [0,255]. As a result, the optimization routines, Stochastic Gradient Descent for LR and Adam for CNN, could converge more quickly and reliably, reducing the chances of getting stuck in poor local minima or suffering from numerical instabilities. In essence, feature scaling fosters a more stable and efficient learning environment for the model. 

\subsection{Classification Techniques}

The following classification methods were implemented:

\subsubsection{Convolutional Neural Network}

Convolutional Neural Networks are a type of deep learning model specifically designed for processing structured grid data, such as images. CNNs exploit spatial hierarchies in data by applying convolutional operations to extract features. These features are then used to perform classification tasks.

A key component of a CNN is the convolution operation. For a 2D input, the convolution is defined as in equation \ref{eq:CNN2Dinput}:

\begin{equation}
    (f * g)(x, y) = \sum_{m=-\infty}^{\infty} \sum_{n=-\infty}^{\infty} f(m, n) \cdot g(x - m, y - n)
    \label{eq:CNN2Dinput}
\end{equation}

Here, \(f(m, n)\) represents the input image, and \(g(x - m, y - n)\) is the filter or kernel. The kernel slides over the image to compute feature maps.

To control the size of the output feature maps, CNNs often use padding and strides. Padding can increase the dimensionality of the output and as such avoid the shrinking that convolutional layers usually apply to the tensor when passing through them. The size of the output feature map, \(D_{out}\), for an input of size \(D_{in}\), a kernel (filter) size \(K\), stride \(S\), and padding \(P\), is calculated as in equation \ref{eq:outputfeaturemapequation}:

\begin{equation}
    D_{out} = \left\lfloor\frac{D_{in} - K + 2P}{S}\right\rfloor + 1
    \label{eq:outputfeaturemapequation}
\end{equation}

The output tensor is then of dimensions $D_{out}\times D_{out}\times C$ where $C$ is the number of channels. Padding is also useful to help capture more information from edge pixels, since it allows filters to capture edge pixels more times as it slides over the image.
The final layers of a CNN are typically fully connected (dense) layers, which map the extracted features to class probabilities using the softmax function in equation \ref{eq:softmaxfunction}:

\begin{equation}
    p(y = c \mid \boldsymbol{x}) = \frac{\exp(z_c)}{\sum_{k=1}^C \exp(z_k)}
    \label{eq:softmaxfunction}
\end{equation}

Here, \(z_c\) represents the output score for class \(c\), and \(C\) is the total number of classes.

CNNs are trained using backpropagation. The loss function cross-entropy, which we use consistently in all of our experiments \ref{eq:Lossfuntion}, guides the optimization of the weights:

\begin{equation}
    L = -\sum_{i=1}^n \sum_{c=1}^C y_{i,c} \log \hat{y}_{i,c}
    \label{eq:Lossfuntion}
\end{equation}

In this equation, \(y_{i,c}\) is the true label, and \(\hat{y}_{i,c}\) is the predicted probability for class \(c\).

CNNs have demonstrated exceptional performance in image classification tasks by automatically learning hierarchical features from raw input data \cite{raschka2022machine}.

\subsubsection{Logistic Regression}

Logistic regression can be used for multiclass classification problems, predicting the probability that an input $\boldsymbol{x}_i$ belongs to one of several classes $y_i \in {0, 1, \dots, K-1}$. It models these probabilities using the softmax function in equation \ref{eq:softmaxfunction}, which outputs values between 0 and 1 that sum to 1 across all classes:

\begin{equation}
p(y_i = k \vert \boldsymbol{x}_i, \boldsymbol{\beta}) = \frac{\exp(\boldsymbol{x}_i^\mathrm{T} \boldsymbol{\beta}k)}{\sum{j=0}^{K-1} \exp(\boldsymbol{x}_i^\mathrm{T} \boldsymbol{\beta}_j)}, \quad \text{for } k = 0, 1, \dots, K-1.
\label{eq:softmaxfunction}
\end{equation}

To estimate the parameters $\boldsymbol{\beta}$, Maximum Likelihood Estimation (MLE) is used. Assuming independent data points, the likelihood function for the dataset $\mathcal{D} = { (\boldsymbol{x}_i, y_i) }$ is described in equation \ref{eq:likelihoodfunction_softmax}:

\begin{equation}
L(\boldsymbol{\beta}) = \prod_{i=1}^n \prod_{k=0}^{K-1} [p(y_i = k \vert \boldsymbol{x}_i, \boldsymbol{\beta})]^{\mathbb{1}(y_i = k)},
\label{eq:likelihoodfunction_softmax}
\end{equation}

where $\mathbb{1}(y_i = k)$ is an indicator function that equals 1 if $y_i = k$ and 0 otherwise.

The log-likelihood function is then expressed as in equation \ref{eq:loglikelihoodfunction_softmax}:

\begin{equation}
\ell(\boldsymbol{\beta}) = \sum_{i=1}^n \sum_{k=0}^{K-1} \mathbb{1}(y_i = k) \log p(y_i = k \vert \boldsymbol{x}_i, \boldsymbol{\beta}).
\label{eq:loglikelihoodfunction_softmax}
\end{equation}

The cost function to minimize in our implementation is the negative log-likelihood, which is equivalent to the cross-entropy loss as shown in equation \ref{eq:cross_entropy_loss}:

\begin{equation}
C(\boldsymbol{\beta}) = -\ell(\boldsymbol{\beta}) = -\sum_{i=1}^n \sum_{k=0}^{K-1} \mathbb{1}(y_i = k) \log p(y_i = k \vert \boldsymbol{x}_i, \boldsymbol{\beta}).
\label{eq:cross_entropy_loss}
\end{equation}

The gradient of the cost function, used for optimization algorithms like gradient descent, is expressed in matrix form as in equation \ref{eq:gradientcostfunction_softmax}:

\begin{equation}
\nabla_{\boldsymbol{\beta}} C(\boldsymbol{\beta}) = -\mathbf{X}^\mathrm{T} (\mathbf{Y} - \mathbf{P}),
\label{eq:gradientcostfunction_softmax}
\end{equation}

where $\mathbf{X}$ is the design matrix with rows $\boldsymbol{x}_i^\mathrm{T}$, $\mathbf{Y}$ is a one-hot encoded matrix of observed labels, $\mathbf{P}$ is a matrix of predicted probabilities $p(y_i = k \vert \boldsymbol{x}_i, \boldsymbol{\beta})$ for all $k$.

Parameters are estimated by minimizing $C(\boldsymbol{\beta})$ using iterative methods, such as gradient descent, since there is no closed-form solution for logistic regression coefficients \cite{Hjorth-Jensen_MachineLearning_2023}.

\subsection{Optimization Techniques}
To explore the optimization the convolutional network performance, we used various techniques described in the following:

\subsubsection{Kernel Size and Number of Filters}

We conducted a grid search with kernel sizes $K = {3, 5, 7}$ and filter counts $F = {8, 16, 32, 64}$. Kernel size determines the spatial extent of the receptive field, controlling how much of the input image each convolution operation captures. Smaller kernel sizes, such as $3\times3$, allow for precise feature extraction and can be stacked to achieve a larger effective receptive field. In contrast, larger kernel sizes, such as $5\times5$ and $7\times7$, capture broader features but may lead to higher computational costs and increased risk of overfitting.

The number of filters $F$ defines the depth of the output tensor, which determines the model's ability to learn diverse features. Increasing the number of filters enhances the network's capacity to capture complex patterns but also raises computational requirements and the potential for overfitting.
\subsubsection{Padding and Pooling:}

We used Max Pooling, which is used to reduce the dimensionality of an input tensor (usually after a convolutional layer). For a kernel size of $n$, scans $n\times n$ grids and selects the highest values respectively. This grid moves over a number of elements equal to the pooling layer's stride between each such scan, and in our implementation using torch.nn.MaxPool2d(), stride is always equal to kernel size such that these filters never overlap. Pooling modifies the dimensions of the tensor passed through it as shown in equation \ref{eq:pooling-dims}.

\begin{equation}
    D_{out} = \left\lfloor\frac {D_{in} -K + 2P}{S}\right\rfloor
    \label{eq:pooling-dims}
\end{equation}


Here $D$ represents the height and width of the input tensor (size = $D\times D \times C$ where $C$ is channels). K, P, and S are the kernel size, padding size, and stride of the pooling layer respectively.

Padding (in convolutional layers) and Pooling are complimentary, increasing and decreasing tensor dimensions respectively. Setting padding ($P$) in a convolutional layer to $\frac {K-1}2$ where $K$ is the kernel size means the output tensor has the same dimensionality (discounting number of channels) as the input tensor, assuming stride $= 1$ (\ref{eq:outputfeaturemapequation}). We test $P = 0,\frac {K-1}2$, and Pooling Kernel Size = $0,2$, where $0$ essentially means no pooling layer in a grid search. These values were chosen because they are common and pooling layers were placed after each convolutional layer.

\subsubsection{Dropout and Activation functions}
We performed a grid search with various dropout layers, and the activation functions ReLU and Leaky ReLU (used for all except the last output) as ReLU is the most commonly used activation function in convolutional neural networks, while leaky ReLU has also proven useful in convolutional networks \cite {activation_functions}. Dropout is implemented in our code using torch.nn.Dropout(). Dropout sets the output of neurons of the preceding layer to zero with a set probability between zero and one. We tested the use of dropout after the first two (of three) linear layers, with probabilities \{0.3, 0.4, 0.5\} (same for both) in, as is similar to one of Raschka's implementations \cite[p. 478]{raschka2022machine}.


\subsection{Validation and Evaluation techniques}

\subsubsection{Cross-Validation with K-Folds}

To assess the models' generalization capability and to avoid overfitting, we employed $K$-fold cross-validation with $K = 5$. In this method, the training data is partitioned into K subsets (folds). The model was trained on $k-1$ folds and validated on the remaining fold. This process was repeated $k$ times, each time with a different fold used for validation. The cross-validation procedure provides an average performance metric, which is more reliable than a single train-test split. The accuracy score were calculated for each fold and then averaged to obtain the final performance metrics. The algorithm is presented with pseudocode in Algorithm 1.


\begin{figure}[H]
    \begin{algorithm}[H]
    \caption{K-fold Cross Validation \cite{K-foldCrossValidation}}
    \label{algo:kfold}
        \begin{algorithmic}[1]
            \Procedure{K-foldCrossValidation}{$model, X, z, nfolds$}
            \State Divide data into K equal folds 
            \For{$k \in \text{range}(0, K)$}
                \State $V \gets \text{Fold}_{k}$ in data
                \State $T \gets \text{data} \setminus V$
                \Comment{Training on data except the validation data}
                \State Train $T$
                \State $Acc_k \gets$ evaluate $V$ with trained model
                \Comment{Accuracy evaluated for one fold}
            \EndFor
            \State $Acc \gets \frac{1}{K} \sum_{k=1}^{K} Acc_k$
            \Comment{Total accuracy is evaluated}
             \EndProcedure
        \end{algorithmic}
    \end{algorithm}
\end{figure}

\subsubsection{Bootstrapping}

In addition to cross-validation, we used non-parametric bootstrap to estimate the stability of our model performance. Our bootstrapped implementation repeatedly resampled the test data with replacement to create multiple bootstrap samples B. For each bootstrap sample B, we evaluated model performance. The bootstrap algorithm is presented with pseudocode in Algorithm 2. This method allowed us to compute confidence intervals for the model performance.
\cite{hastie2009elements}

\subsubsection{Confidence Intervals}
After running the boostrap algorithm, we constructed a 95 \% confidence interval of our model accuracy. A 95\% confidence interval is a range of values that, with 95\% probability, contains the true unknown value of a parameter\cite{james2013introduction}. The lower and upper limits of our confidence interval were calculated by taking the 2.5th and 97.5th percentiles of the bootstrapped accuracy estimates.

\begin{figure}[H]
    \begin{algorithm}[H]
    \caption{Bootstrap Algorithm}
    \label{algo:bootstrap}
        \begin{algorithmic}[1]
            \Procedure{Bootstrapping}{$B$, model, data}
            \For{$b = 1$ to $B$}
                \State $\mathcal{D}^{(b)}_{\text{test}} \gets$ Sample $n$ data points from $\mathcal{D}$ with replacement
                \State Evaluate the model on $\mathcal{D}^{(b)}_{\text{test}}$ and record the estimated accuracy
            \EndFor
            \State Construct confidence intervals by taking the percentiles of the boostraped estimates.
            \EndProcedure
        \end{algorithmic}
    \end{algorithm}
\end{figure}

\subsection{Implementation}
\subsubsection{CNN}
The CNN class was implemented using the \texttt{PyTorch} library \cite{Paszke2019}, drawing inspiration from the "Training a Classifier" tutorial in the PyTorch documentation \cite{pytorch_cifar10_tutorial}. After studying this example as a proper way to code a CNN with PyTorch, we created a class capable of accommodating various network structures, enabling us to perform grid searches for the optimal configuration. For the initial parameters of our grid search experiments, we partially adopted the values from Raschka's example of a CNN for the MNIST dataset \cite{raschka2022machine}. However, we opted to limit the number of epochs to ten since Raschka's validation curves indicated convergence after five epochs. 

\subsubsection{Logistic Regression}
The logistic regression was also implemented using the \texttt{PyTorch} library. The original inspiration on how to set up Logistic Regression were found with help from Antropic's Claude 3.5 Sonnet AI model \cite{anthropic_claude_3_5_sonnet}. The code was however modified as we saw we could use the same structure as our CNN model. Based on initial experiments, we selected a learning rate range from \( 10^{-5}\) to \( 10^0 \), batch sizes in the range from 16 to 256, and epochs in the range from 5 to 25.

\subsubsection{Selection Criterion}
Due to our limited computational resources and the large size of our training dataset, it was not feasible to test every possible configuration of our model architecture and hyperparameters. Therefore, we opted to start with informed initial values for our hyperparameters based on trusted sources, rather than selecting them randomly. After each grid search, we iteratively replaced the initial parameters with the best-performing ones. Our criterion for selection was to choose the parameter that maximized cross-validated accuracy. In case of a tie between parameters, we always selected the value that represented a simpler model structure and incurred lower computational costs. It is worth noting that we only suggested values within a range that preliminary experiments indicated would align with our computational resources.
\\
\\
Raschka \ inspired the choice of initial values in our convolutional neural networks cite{raschka2022machine}. We used 10 or 20 epochs (as specified), a batch size of 64, a convolutional layer stride of 1, and a pooling layer (where applicable) stride of 1 and padding of 0. The following steps summarize the implementation process of our experiment, with the initial values for LogReg presented in Table \ref{tb:loginitial} and for CNN presented in Table \ref{tb:initialvalues}:

\begin{table}[H]
 \centering
    \caption{Table with inital values for LogReg before hyper parameter tuning.}
\begin{tabular}{|l|l|l|l|l|}
\hline
                        & \textbf{Optimizer} & \textbf{Learning Rate}    & \textbf{Batch Size} & \textbf{Epochs} \\ \hline
\textbf{Initial Values} & SGD                & Best of first grid search & 64                  & 10              \\ \hline
\end{tabular}
\label{tb:loginitial}
\end{table}

\begin{table}[H]
    \centering
    \caption{Table with inital values for CNN before hyper parameter tuning, adapted from Raschka \cite{raschka2022machine}}
    
\begin{tabular}{|p{2.5cm}|p{2cm}|l|p{1.5cm}|p{2cm}|p{2.5cm}|p{3cm}|}
\hline
                       &\textbf{Learning rate} &  \textbf{Optimizer} & \textbf{Kernel size} & \textbf{Filter Numbers}& \textbf{Padding} (conv. layer) & \textbf{Pooling kernel size} (After each conv. layer) \\ \hline
\textbf{Inital values}                 & 0.001                  & Adam               & 5                    & (32, 64)     & 0
            & 2\\ \hline
\end{tabular}
\label{tb:initialvalues}
\end{table}

\begin{enumerate}
    \item \textbf{Data Loading and Splitting:} Load the MNIST data set and split it into train and test sets using     \texttt{torchvision.datasets}. 
    \item \textbf{Tune Baseline Model}: We use stochastic gradient decent as optimizer and negative log-likelihood for Logistic Regression.Tune our Logistic Regression model using cross-validation.
    \item \textbf{Tune CNN}: Grid search the hyperparameters of our CNN in the following order, using cross-validation:
    \begin{enumerate}
        \item \textbf{Number of Linear and Convolutional Layers} Test one convolutional layer followed by two linear layers against two convolutional layer followed by three linear layers.
        \item \textbf{Kernel Size and Number of Filters}
        We test Kernel Size: \{3, 5, 7\} and Number of Filters: \{(8,16), (16, 32), (32,64)\}.
        \item \textbf{Pooling and Padding}
        We test padding in convolutional layers of values $\{0,\frac {K-1}2\}$, where $K$ is the convolutional layer kernel size, and Pooling Kernel sizes: \{0,2\}.
        \item \textbf{Dropout rates and Activation Functions}
        We perform a grid search with the dropout rates \{0.3, 0.5\} and Activation Functions: \{ReLU, Leaky ReLU\}.
    \end{enumerate}
    \item \textbf{Model Training:} Train CNNs and Logistic Regression on the training data with their respective validated hyper parameters. 
    \item \textbf{Testing:} Assess the final model performance on the test set. 
\end{enumerate}


\subsection{Performance Metrics}

To measure the performance of the classification problem, we use the accuracy score in equation \ref{eq:AccuracyScore}: \begin{equation}
    Accuracy = \frac{\sum_{i=1}^n I(t_i = y_i)}{n},
    \label{eq:AccuracyScore}
\end{equation}
where the number of correctly guessed targets $t_i$ is divided by the total number of targets.

\subsection{Reproducibility}
To ensure the reproducibility of our computational experiments, we have taken the following steps:
\begin{itemize}
    \item Use a well known public data set with published benchmarked accuracies
    \item Present all final model parameters, as long as values tested during tuning of hyper parameters
    \item Use config files for the hyper parameters to make sure we were consistent across different scripts and make our code more readable
    \item Seeded everything with the initial seed from a separate config file.
    \item Validated our tuning process with cross-validation, where we vary the seeds across different runs to validate our results further. 
    \item Used bootstrapped uncertainty measures for final model, where we vary the seeds across different runs. 
\end{itemize}

\subsection{Testing}
We tested our implemention by comparing the performance of our model with the same initial values and structure as that of known benchmarked accuracies on the MNIST data set \cite{raschka2022machine} and \cite{lecun2015deep}. As a bonus test, we have made ChatGPT review our code, and ask it do describe what the scripts do. One could also argue that we have made our implementation more robust by having it reviewed by all members of the group, which in our experience increased the chance of catching errors. 

\subsection{Large Language Models}

With inspiration from FYS-STK4155 project 1 \cite{bore2023fysstk4155project1} we summarize our use of large language models: 
We have been encouraged in the group sessions to use ChatGPT \cite{openai2023chatgpt} in writing this report. We have done so in various ways. The abstract was created by first writing the whole paragraph, then sending it to ChatGPT with the prompt "Can you rewrite this with better and more concise language? Keep the references as they are and only keep the most important equations to explain the methods:". We have then read through the suggestion closely to make sure the values and content are the same as before. For the introduction and parts of the method we have asked ChatGPT to write the whole paragraph based on selected criteria and a template. After we have read through and done corrections to fit the overall report. We hope that this makes it easier for the reader to follow our discussion, especially when we discuss the figures. Screenshots from conversations with ChatGPT are uploaded in a folder on our GitHub. We have also used Github Copilot as an integrated tool \cite{github_copilot}. Antropic's Cloude 3.5 Sonnet was used for inspiration for the implementation of the original Logistic Regression code, but it was later modified by us to be on the same format as the CNN model. \cite{anthropic_claude_3_5_sonnet}. 


\subsection{Other Tools}

We used the software Overleaf to write this report. To do basic mathematical operations we used the NumPy package \cite{harris2020numpy}. Plotting our results was done with the Matplotlib package \cite{hunter-2007matplotlib}. The code for the project can be found at: \url{https://github.com/irisbore/FYS-STK4155_project3.git}.
