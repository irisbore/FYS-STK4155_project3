The primary focus in this study was on implementing and comparing the performance of different CNNs with various layers, nodes and activation functions on the MNIST dataset. These was afterwards compared with Logistic Regression (LR) for 5 different learningrates, batchsizes and epochs. To ensure the robustness and reliability of our results, we utilized data preprocessing and optimization techniques, including train-test splitting, feature scaling and bootstrapping. 

\subsection{Data Generation and Preprocessing}

\subsubsection{Train-Test Split}

 We loaded the MNIST dataset and divided it into two subsets: a training set and a test set. The training set was used during the optimization process, where the model parameters were iteratively updated to minimize the loss function. On the other hand, the test set remained completely untouched until the final evaluation phase. This separation allowed us to assess the model’s ability to generalize to new, unseen data. By ensuring that model selection and hyperparameter tuning were performed only on training data, and final performance checks occur on the test data, we gained a more realistic measure of the model’s predictive power and robustness.

\subsubsection{Feature Scaling}

We applied a normalization transformation to the MNIST images, scaling their pixel intensity values so that they had a mean of 0.5 and a standard deviation of 0.5. This normalization ensured that the input features (pixels) lied within a more uniform and manageable range. As a result, the optimization routines, such as stochastic gradient descent or Adam, could converge more quickly and reliably, reducing the chances of getting stuck in poor local minima or suffering from numerical instabilities. In essence, feature scaling fosters a more stable and efficient learning environment for the model.

\subsection{Classification Techniques}

The following classification methods were implemented:

\subsubsection{Convolutional Neural Network}

Convolutional Neural Networks are a type of deep learning model specifically designed for processing structured grid data, such as images. CNNs exploit spatial hierarchies in data by applying convolutional operations to extract features. These features are then used to perform classification tasks.

A key component of a CNN is the convolution operation. For a 2D input, the convolution is defined as:

\begin{equation}
    (f * g)(x, y) = \sum_{m=-\infty}^{\infty} \sum_{n=-\infty}^{\infty} f(m, n) \cdot g(x - m, y - n)
\end{equation}

Here, \(f(m, n)\) represents the input image, and \(g(x - m, y - n)\) is the filter or kernel. The kernel slides over the image to compute feature maps.

To control the size of the output feature maps, CNNs often use padding and strides. The size of the output feature map, \(O\), for an input of size \(I\), a filter size \(F\), stride \(S\), and padding \(P\), is calculated as:

\begin{equation}
    O = \frac{I - F + 2P}{S} + 1
\end{equation}

Pooling layers, such as max-pooling, are used for dimensionality reduction while preserving essential features. For example, max-pooling selects the maximum value within a specified window, reducing the spatial dimensions of the feature map.

The final layers of a CNN are typically fully connected (dense) layers, which map the extracted features to class probabilities using a softmax function:

\begin{equation}
    p(y = c \mid \boldsymbol{x}) = \frac{\exp(z_c)}{\sum_{k=1}^C \exp(z_k)}
\end{equation}

Here, \(z_c\) represents the output score for class \(c\), and \(C\) is the total number of classes.

CNNs are trained using backpropagation, where the loss function, such as cross-entropy, guides the optimization of weights:

\begin{equation}
    L = -\sum_{i=1}^n \sum_{c=1}^C y_{i,c} \log \hat{y}_{i,c}
\end{equation}

In this equation, \(y_{i,c}\) is the true label, and \(\hat{y}_{i,c}\) is the predicted probability for class \(c\).

CNNs have demonstrated exceptional performance in image classification tasks by automatically learning hierarchical features from raw input data \cite{raschka2022machine}.


\subsubsection{Logistic Regression}

Logistic regression is used for binary classification problems, predicting the probability that an input $\boldsymbol{x}_i$ belongs to class $y_i \in \{0, 1\}$. It models this probability using the sigmoid function, which outputs values between 0 and 1:

\begin{equation}
    p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta}) = \frac{\exp(\boldsymbol{x}_i^\mathrm{T} \boldsymbol{\beta})}{1 + \exp(\boldsymbol{x}_i^\mathrm{T} \boldsymbol{\beta})}
\end{equation}

The probability of belonging to class $y_i = 0$ is:

\begin{equation}
    p(y_i = 0 \vert \boldsymbol{x}_i, \boldsymbol{\beta}) = 1 - p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta})
\end{equation}

To estimate the parameters $\boldsymbol{\beta}$, Maximum Likelihood Estimation (MLE) is used. Assuming independent data points, the likelihood function for the dataset $\mathcal{D} = \{ (\boldsymbol{x}_i, y_i) \}$ is:

\begin{equation}
    L(\boldsymbol{\beta}) = \prod_{i=1}^n [p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta})]^{y_i} [1 - p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta})]^{1 - y_i}
\end{equation}

The log-likelihood function is then:

\begin{equation}
    \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ y_i \log p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta}) + (1 - y_i) \log \left( 1 - p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta}) \right) \right]
\end{equation}

The cost function to minimize is the negative log-likelihood:

\begin{equation}
    C(\boldsymbol{\beta}) = -\ell(\boldsymbol{\beta})
\end{equation}

The gradient of the cost function, used for optimization algorithms like gradient descent, is expressed in matrix form as:

\begin{equation}
    \nabla_{\boldsymbol{\beta}} C(\boldsymbol{\beta}) = -\mathbf{X}^\mathrm{T} (\boldsymbol{y} - \boldsymbol{p})
\end{equation}

where $\mathbf{X}$ is the design matrix with rows $\boldsymbol{x}_i^\mathrm{T}$,
 $\boldsymbol{y}$ is the vector of observed labels,
 $\boldsymbol{p}$ is the vector of predicted probabilities $p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta})$.

Parameters are estimated by minimizing $C(\boldsymbol{\beta})$ using iterative methods since there is no closed-form solution for logistic regression coefficients \cite{Hjorth-Jensen_MachineLearning_2023}.


\subsection{Implementation}

The CNNs were implemented using the \texttt{PyTorch} library \cite{Paszke2019} with inspiration from the tutorial "Training a Classifier" \cite{pytorch_cifar10_tutorial}. We mainly changed the code to include gridsearch and to include binary cross-entropy as loss function and adam as optimizer. The gridsearch was done over various layers, nodes and activation functions.( \jonas{Vær vennlig å inkluder spesifik range på layers/noder og spesifikke aktiverings funksjoner}). The logistic regressioin was also implemented using the \texttt{PyTorch} library but with help from Antropic's Claude 3.5 Sonnet ai model \cite{anthropic_claude_3_5_sonnet}. Based in initial experiments, we selected a learning rate range from \( 10^{-5}\) to \( 10^0 \), batch sizes in range from 16 to 256 and epochs in range from 5 to 25. The following steps summarize the implementation process:

\begin{enumerate}
    \item \textbf{Data Loading and Splitting:} Load the MNIST data set and split it into train and test sets using \texttt{torchvision.datasets}. 
    \item \textbf{Feature scaling:} Apply \texttt{transforms.Compose([transforms.ToTensor(),
    transforms.Normalize((0.5), (0.5))])}, to normalize the pixel intensity values to mean 0.5 and standard deviation 0.5. 
    \item \textbf{Model Training:} Train CNNs and Logistic Regression on the training data.
    \item \textbf{Parameter Tuning:} Use a range of learning rates, batchsizes and epochs to find optimal Accuracy scores. 
    \item \textbf{Testing:} Assess the final model performance on the testing set.
\end{enumerate}


\subsection{Performance Metrics}

To measure the performance of the classification problem, we use the accuracy score: \begin{equation}
    Accuracy = \frac{\sum_{i=1}^n I(t_i = y_i)}{n},
\end{equation}
where the number of correctly guessed targets $t_i$ is divided by the total number of targets.


\subsection{Large Language Models}

With inspiration from FYS-STK4155 project 1 \cite{bore2023fysstk4155project1} we summarize our use of large language models: 
We have been encouraged in the group sessions to use ChatGPT \cite{openai2023chatgpt} in writing this report. We have done so by first writing the whole paragraph, then sending it to ChatGPT with the prompt "Can you rewrite this with better and more concise language? Keep the references as they are and only keep the most important equations to explain the methods:". We have then read through the suggestion closely to make sure the values and content are the same as before. We hope that this makes it easier for the reader to follow our discussion, especially when we discuss the figures. Screenshots from conversations with ChatGPT are uploaded in a folder on our GitHub. We
have also used Github Copilot as an integrated tool \cite{github_copilot}. Antropic's Cloude 3.5 Sonnet was used to create the logistic regression code \cite{anthropic_claude_3_5_sonnet}. 


\subsection{Other Tools}

We used the software Overleaf to write this report. To create the dataset along with doing basic mathematical operations we used the NumPy package \cite{harris2020numpy}. Plotting our results was done with the Matplotlib package \cite{hunter-2007matplotlib}. The code for the project can be found at: \url{https://github.com/irisbore/FYS-STK4155_project3.git}.
