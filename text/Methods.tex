In this study, we did a gridsearch of a Convolutional Neural Network and compared the classification accuracy with logistic regression on the MNIST dataset. The primary focus was on implementing and comparing the performance of different Convolutional Neural Networks (CNN) with various layers, nodes and activation functions. These was afterwards compared with Logistic Regression (LR) for 5 different learningrates, batchsizes and epochs. To ensure the robustness and reliability of our results, we utilized data preprocessing and optimization techniques, including train-test splitting, feature scaling and bootstrapping. 

\subsection{Data Generation and Preprocessing}

\subsubsection{Train-Test Split}

The train-test split is a fundamental step in machine learning workflows to ensure that models are evaluated on data that they have not seen during training. In this project, we load the MNIST dataset and divide it into two subsets: a training set and a test set. The training set is used during the optimization process, where the model parameters are iteratively updated to minimize the loss function. On the other hand, the test set remains completely untouched until the final evaluation phase. This separation allows us to assess the model’s ability to generalize to new, unseen data. By ensuring that model selection and hyperparameter tuning are performed only on training data, and final performance checks occur on the test data, we gain a more realistic measure of the model’s predictive power and robustness.

\subsubsection{Feature Scaling}

Feature scaling is an essential preprocessing step that helps stabilize and accelerate the model training process. In this work, we apply a normalization transformation to the MNIST images, scaling their pixel intensity values so that they have a mean of 0.5 and a standard deviation of 0.5. This normalization ensures that the input features (pixels) lie within a more uniform and manageable range. As a result, the optimization routines, such as stochastic gradient descent or Adam, can converge more quickly and reliably, reducing the chances of getting stuck in poor local minima or suffering from numerical instabilities. In essence, feature scaling fosters a more stable and efficient learning environment for the model.

\subsection{Classification Techniques}

The following classification methods were implemented:


\subsubsection{Logistic Regression}

Logistic regression is used for binary classification problems, predicting the probability that an input $\boldsymbol{x}_i$ belongs to class $y_i \in \{0, 1\}$. It models this probability using the sigmoid function, which outputs values between 0 and 1:

\begin{equation}
    p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta}) = \frac{\exp(\boldsymbol{x}_i^\mathrm{T} \boldsymbol{\beta})}{1 + \exp(\boldsymbol{x}_i^\mathrm{T} \boldsymbol{\beta})}
\end{equation}

The probability of belonging to class $y_i = 0$ is:

\begin{equation}
    p(y_i = 0 \vert \boldsymbol{x}_i, \boldsymbol{\beta}) = 1 - p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta})
\end{equation}

To estimate the parameters $\boldsymbol{\beta}$, Maximum Likelihood Estimation (MLE) is used. Assuming independent data points, the likelihood function for the dataset $\mathcal{D} = \{ (\boldsymbol{x}_i, y_i) \}$ is:

\begin{equation}
    L(\boldsymbol{\beta}) = \prod_{i=1}^n [p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta})]^{y_i} [1 - p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta})]^{1 - y_i}
\end{equation}

The log-likelihood function is then:

\begin{equation}
    \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ y_i \log p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta}) + (1 - y_i) \log \left( 1 - p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta}) \right) \right]
\end{equation}

The cost function to minimize is the negative log-likelihood:

\begin{equation}
    C(\boldsymbol{\beta}) = -\ell(\boldsymbol{\beta})
\end{equation}

The gradient of the cost function, used for optimization algorithms like gradient descent, is expressed in matrix form as:

\begin{equation}
    \nabla_{\boldsymbol{\beta}} C(\boldsymbol{\beta}) = -\mathbf{X}^\mathrm{T} (\boldsymbol{y} - \boldsymbol{p})
\end{equation}

where $\mathbf{X}$ is the design matrix with rows $\boldsymbol{x}_i^\mathrm{T}$,
 $\boldsymbol{y}$ is the vector of observed labels,
 $\boldsymbol{p}$ is the vector of predicted probabilities $p(y_i = 1 \vert \boldsymbol{x}_i, \boldsymbol{\beta})$.

Parameters are estimated by minimizing $C(\boldsymbol{\beta})$ using iterative methods since there is no closed-form solution for logistic regression coefficients \cite{Hjorth-Jensen_MachineLearning_2023}.


\subsection{Implementation}

Gradient descent and Stochastic gradient descent were implemented with our own code. The neural network was implemented firstly with our own code and after with the \texttt{PyTorch} library \cite{Paszke2019} for comparison. We used Mean squared error as the loss function and Adam as the Optimizer for the regression task. For classification, we used the Adam optimizer in all cases, and binary cross entropy as the loss function when sigmoid was the last layer's activation function. When Softmax was the last layer's activation function we used cross entropy as loss function. Lastly, the logistic regression was implemented with our own code before we created the same model with \texttt{scikit-learn} library \cite{scikit-learn}. Based on initial experiments, we selected a learning rate range from \( 10^{-5} \) to \( 10^0 \) and epoch counts between 900 and 1000 as a standard for all models, as these parameters yielded the most favorable balance between model accuracy and computational efficiency. The following steps summarize the implementation process:

\begin{enumerate}
    \item \textbf{Data Generation:} Generate synthetic second-order polynomial data with added Gaussian noise. Load real data and extract \texttt{"radius\_mean"} and \texttt{"texture\_mean"} as features. 
    \item \textbf{Data Splitting \& feature scaling:} Split the data into training and testing sets using \texttt{train\_test\_split}. Apply \texttt{StandardScaler} to standardize the features.
    \item \textbf{Model Training:} Train FFNN and Logistic Regression on the training data.
    \item \textbf{Parameter Tuning:} Use a range of epochs and learning rates to find optimal MSE, R2, and Accuracy scores. Test various $\lambda$ values for the Logistic Regression model.
    \item \textbf{Testing:} Assess the final model performance on the testing set.
\end{enumerate}


\subsection{Performance Metrics}

To measure the performance of the classification problem, we use the accuracy score: \begin{equation}
    Accuracy = \frac{\sum_{i=1}^n I(t_i = y_i)}{n},
\end{equation}
where the number of correctly guessed targets $t_i$ is divided by the total number of targets.


\subsection{Large Language Models}

With inspiration from FYS-STK4155 project 1 \cite{bore2023fysstk4155project1} we summarize our use of large language models: 
We have been encouraged in the group sessions to use ChatGPT \cite{openai2023chatgpt} in writing this report. We have done so by first writing the whole paragraph, then sending it to ChatGPT with the prompt "Can you rewrite this with better and more concise language? Keep the references as they are and only keep the most important equations to explain the methods:". We have then read through the suggestion closely to make sure the values and content are the same as before. We hope that this makes it easier for the reader to follow our discussion, especially when we discuss the figures. Screenshots from conversations with ChatGPT are uploaded in a folder on our GitHub. We
have also used Github Copilot as an integrated tool \cite{github_copilot}.


\subsection{Other Tools}

We used the software Overleaf to write this report. To create the dataset along with doing basic mathematical operations we used the NumPy package \cite{harris2020numpy}. Plotting our results was done with the Matplotlib package \cite{hunter-2007matplotlib}. We implemented the methods with inspiration from the lecture notes in FYS-STK4155 \cite{Hjorth-Jensen_MachineLearning_2023}. The code for the project can be found at: \url{https://github.com/irisbore/FYS-STK4155_project3.git}.
